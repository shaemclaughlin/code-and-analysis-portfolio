# Code and Analysis Portfolio

Hello, welcome to my code and analysis portfolio. I am Master of Science in Health Data Science student at the University of California, San Francisco, where I have developed a capstone project exploring the potential and limitations of genomic language models (gLMs). Prior to this, I was a medical student at the University of Melbourne, Australia. In Fall 2025, I will begin as a PhD in Bioengineering student at Stanford University. 

## My Interests
My primary research interest lies at the intersection of deep learning and genomics, with a focus on leveraging large language models (LLMs) to decode the complexities of gene regulation and non-coding DNA.

Understanding how genes are expressed and regulated is crucial for comprehending biological development, health, and the origins of disease. The sheer scale and complexity of genomic data, especially the intricate rules governing gene regulation across vast stretches of DNA, present significant hurdles for traditional analysis.

This is where I see the transformative potential of deep learning. Inspired by their success in natural language processing, LLMs offer powerful capabilities uniquely suited to biological sequence analysis. Their ability to learn complex patterns and long-range dependencies directly from sequence data aligns remarkably well with the biological reality of regulatory elements influencing genes from afar.

I am particularly interested in applying and developing LLM-based approaches for:

* Predicting the function of DNA sequences, especially within the non-coding regions that orchestrate gene regulation.
* Identifying regulatory elements like enhancers and promoters and understanding their sequence determinants.
* Modeling the impact of genetic variants on gene regulation and function, contributing to insights into disease mechanisms and personalized medicine.

Beyond applying this methods, I have a strong interest in mechanistic interpretability - a new field dedicated to reverse-engineering neural networks like transformers to understand how they perform computations. Rather than treating models as "black boxes", mechanistic interpretability aims to map their internal components, such as attention heads and circuits, to human-understandable algorithms and concepts. This pursuit is driven not only by scientific curiosity to understand the emergent reasoning processes within these complex systems but also by the critical need to ensure the safety, reliability, and alignment of powerful AI models. Understanding the internal mechanisms of the LLMs used in genomics is crucial for validating the biological insights they generate and building trust in their predictions.

## Skills Summary
### Programming Languages
* Python
* R
* SQL
